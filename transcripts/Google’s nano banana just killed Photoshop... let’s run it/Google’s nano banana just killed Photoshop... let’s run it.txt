A few days ago, image editing changed forever. Google released Gemini Flash 2.5 image, coincidentally with the same nickname my ex-girlfriend gave me, Nano Banana. That has everyone buying puts on Adobe, because Photoshop is officially dead. Instead of learning how to use all these antique tools, you can now just prompt Nano Banana for changes and it's able to deliver any photo alterations you can imagine. And most importantly, while maintaining the consistency of the original image. If you're an animator or graphic designer, this can unlock massive productivity gains. Like if you wanted to steal some meat canyon art and make it do something else, you can now accomplish that with a single prompt. No talent necessary. Google is finally back to its roots building cool stuff, and Flash 2.5 is a tool that makes stable diffusion look like Microsoft Paint. In today's video, we'll take a look at all of its capabilities and find out if our Adobe puts will actually print. It is August 29th, 2025, and you're watching The Code Report. Not only is Nano Banana an exceptional image model that's already at the top of the LM Arena leaderboard, but it's also a great way to get your image but it's also extremely fast and affordable, costing only 3.9 cents per image via the API. And hypothetically, Google has also trained an even more powerful Grande Banana, but it's unlikely the common man will ever get access to it. The upgrade that most people are talking about though is character consistency. If you start with an image of a person or pet for example, the model can blend it with a different image or make minor changes to it without noticeably altering the original character. Or multiple characters and objects like this guy did by blending 13 different images together. If you're an animator or graphic designer, you can also use Nano Banana to create a character that looks like a human. That means if you want a LinkedIn headshot of yourself wearing a nice suit, but can't afford to buy an actual suit, Nano Banana has you covered. In addition, if you work for the Shadow government, it can be used to implement Mandela effects, like removing the cornucopia from the Fruit of the Loom logo. That's cool, but if you're a wannabe game developer, one of the main showstoppers is the insane amount of time it takes to build up a collection of good looking assets for all your characters and animations. Well now, if you start with a base character, you can prompt all the different positions required for an animation. Or better yet, just have Gemini, the main character, and the main character in the game. You can create a character that looks like a human, or a character that looks like a human, or a character that looks like a human, or a character that looks like a human. And you can also have Gemini generate an entire sprite for you with all of them in a single prompt. Keep in mind though, if you go this route, you'll have to disclose any AI assets used when you publish on Steam. And you can't lie about it, because anything generated with Google has an invisible watermark called SynthID. What's kind of crazy about this model though, is that it also has an understanding of the real world. Like if you point to a spot on Google Maps and ask what a person would see there, it can generate a realistic photo. Or if you're a visual learner, you can sketch with it step by step. Like you could use the Sketch tool to map out your AWS infrastructure. And it's a great way to get the most out of your It's also pretty decent at handling text, so you can easily turn your images into lame Instagram ads or memes. But it's still not perfect, so now let's talk about the bad stuff. As you can see in my ad here, it has a tendency to add extra characters to certain words. In addition, I found that it often didn't follow my prompt exactly and just kind of did its own thing. Or in many cases, it just ignored the prompt and did nothing at all. And I think even the character consistency is a bit overhyped. I tried it on a bunch of different images of real humans, and it still has that obvious AI uncanny valley look to it. And finally, I found that it's a little bit more as expected as a Google product, it's highly censored. Trying to generate anything that's not safe for work is not going to happen. But the best way to get the most out of these AI tools is to learn how they work under the hood. And you can do that for free with Brilliant, the sponsor of today's video. Their How AI Works course teaches you how to build a functioning language model from scratch, and lets you experiment with things like feature vectors to edit facial expressions and images. This hands-on approach to learning helps you master challenging concepts over time, and is proven to be six times more effective than the other. So if you're a visual learner, you should definitely try Brilliant. Try everything Brilliant has to offer for free by visiting brilliant.org slash fireship, or scan the QR code on screen to get 20% off their premium annual subscription, which gives you unlimited daily access to everything on Brilliant. This has been The Code Report, thanks for watching, and I will see you in the next one.